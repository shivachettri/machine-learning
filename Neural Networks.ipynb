{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "Neural Network is a machine learning model to classify data.\n",
    "\n",
    "Example: If we want to classify hand-written number into digital form. We will take the image of the hand-written digit pass it as an input (as pixels) and feed it to the neural network. The processing happend between input and output layer called hidden layers. Finally, the output layer consisting of 10 nodes (one each for each digit from 0 to 9) will classify the input hand-written image pixels into digit based on the node that activated at most (i.e, if node 3 is mostly activated than it means the digit is 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Function\n",
    "Sigmoid function takes in mathematical function that takes in input and outputs either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Gradient\n",
    "This function is used to minimize error while back propogating through the network. We do that by calculating the derivative of 'x' at each activation in each layer.\n",
    "Error Gradient/Slope is given as x * (1 â€“ x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_gradient(x):\n",
    "    return x * (1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data\n",
    "We will design a 3 layer neural network, with: input layer, 1 hidden layer and  output layer.\n",
    "- Input Layer\n",
    "- Output Layer\n",
    "- The 3 layers are connected with weights. Thus, to connect input layer to hidden layer one set of weight is required and to connect hidden layer to output layer another set of weights is required. The weight are called synapsis and are randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = np.array([[0.4, 0.3, 0.1],\n",
    "                        [1.0, 0.4, 0.4],\n",
    "                        [0.0, 1.0, 0.8]])\n",
    "\n",
    "output_layer = np.array([[0],\n",
    "                        [0],\n",
    "                        [1]])\n",
    "\n",
    "np.random.seed(1)\n",
    "weights_1 = 2 * np.random.random((3,4)) - 1\n",
    "weights_2 = 2 * np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "The motto behind building a Neural Network is to find weights that when plugged to a neural network can classify input correctly.\n",
    "\n",
    "Before training, we randomly initialize weights that connect all the layers. \n",
    "\n",
    "With randomly initilized weight, we will calculate dot product of input layer and weights connecting input layer and hidden layer which is passed to sigmoid function to calculate activations for hidden layer. Similary, the output layer is calculated. Next, the error is calculated compared to desired output.\n",
    "\n",
    "Next, we will do 'back propogation'. To do so, we calculate the derivate of calculated output layer and multiply with the output layer error to get the output layer delta (adjusting weights) used later to adjust weights connecting output layer and hidden layer. Then, hidden layer error is calcuated taking dot product of weight connect output layer and hidden layer with delta, which is than used to similary to calcuate hidden layer delta.\n",
    "\n",
    "Then, the weights are adjusted. The whole process is repeated for number of times to find weight that is close to weights supplied as output layer above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.5103155049500878\n",
      "Error: 0.003311791991183617\n",
      "Error: 0.0022739887619340404\n",
      "Error: 0.0018290290995867549\n",
      "\n",
      "OUTPUT LAYER:\n",
      "[[2.07709076e-03]\n",
      " [5.76043072e-05]\n",
      " [9.97429249e-01]]\n",
      "\n",
      "WEIGHTS:\n",
      "[[ 4.40196274  0.8957048   2.540444   -3.80914267]\n",
      " [-1.6526529  -1.49514764 -1.59725686  1.1607846 ]\n",
      " [-1.87813772 -0.63999129 -1.6505048   2.38679322]]\n",
      "[[-6.39283219]\n",
      " [-1.55583673]\n",
      " [-4.46493535]\n",
      " [ 6.94364049]]\n"
     ]
    }
   ],
   "source": [
    "#Hyper-Parameters\n",
    "noIterations = 100000\n",
    "\n",
    "\n",
    "for j in range(noIterations):    \n",
    "    #activations\n",
    "    hidden_layer = sigmoid_function(np.dot(input_layer, weights_1))\n",
    "    calculated_output_layer = sigmoid_function(np.dot(hidden_layer, weights_2))\n",
    "    \n",
    "    #error\n",
    "    output_layer_error = output_layer -  calculated_output_layer\n",
    "    \n",
    "    #back propogation\n",
    "    output_layer_delta = output_layer_error * error_gradient(calculated_output_layer)\n",
    "    hidden_layer_error = output_layer_delta.dot(weights_2.T)\n",
    "    hidden_layer_delta = hidden_layer_error * error_gradient(hidden_layer)\n",
    "    \n",
    "    #adjust weight\n",
    "    weights_2 += hidden_layer.T.dot(output_layer_delta)\n",
    "    weights_1 += input_layer.T.dot(hidden_layer_delta)\n",
    "    \n",
    "    \n",
    "    #Uncomment following line to see error reducing progress every 25000 steps\n",
    "    #if(j % 25000) == 0:\n",
    "        #print(\"Error:\",  str(np.mean(np.abs(output_layer_error))) )\n",
    "        \n",
    "    \n",
    "print(\"\\nOUTPUT LAYER:\")\n",
    "print(calculated_output_layer)\n",
    "\n",
    "print(\"\\nWEIGHTS:\")\n",
    "print(weights_1)\n",
    "print(weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "After training the Neural Network with our dataset, we can see that calculate output layer is pretty close to our supplied actual output layer with least error.\n",
    "\n",
    "The designed Neural Network model is trained to predict correct output. Meaning, we can plug in the final weights into a neural network to correctly classified using input data similar to training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
