{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Linear Regression is a Machine Learning model to predict a line that best fit the given data, \n",
    "such that, if new data points are given, we can make prediction based on the line drawn to define dataset.\n",
    "\n",
    "Basically, we are suppose to find the slope(m) and intercept(b) in a equation of a line: y = mx + b.\n",
    "\n",
    "NOTE: We have easier method for figure the value of m and b, however, we will perform Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy is a python package for scientific computing.\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Function\n",
    "Error Function tell us how correct is the value for slope(m) and intercept(b).\n",
    "\n",
    "Mathematically, Error Function = (y - (m * x + b)). Calculating 'y' as '(m * x + b)' and subtracting it with actual value of 'y'.\n",
    "\n",
    "The function take the value of slope(m) and intercept(b) to apply to error funcation through all data points and calculating the Average Squared Error for more precise error, and finally, the average error is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_function(b, m, dataPoints):\n",
    "    error = 0\n",
    "    \n",
    "    for i in range(0,len(dataPoints)):\n",
    "        x = dataPoints[i,0]\n",
    "        y = dataPoints[i,1]\n",
    "        \n",
    "        error += (y - (m * x + b))**2\n",
    "        \n",
    "    averageError = error / float(len(dataPoints))\n",
    "    return averageError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "Gradient Descent is a process of finding the best fitting value (local maxima) of slope(m) and intercept(m).\n",
    "\n",
    "Mathematically, gradient descent is a partial derivative of error function. Therefore,\n",
    "Partial Derivative w.r.t. slope(m): -(2/n) * (y - (m  * x) + b)\n",
    "Partial Derivative w.r.t. intercept(m): -(2/n) * x * ((y - (m * x)) + b)\n",
    "\n",
    "The following function loop through all the datapoints and calculate the gradient for slope and intercept which is later multipled with learning rate and the actual slope and intercept value is adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(b, m, dataPoints, learning_rate):\n",
    "    gradientB = 0\n",
    "    gradientM = 0\n",
    "    \n",
    "    n = float(len(dataPoints))\n",
    "    \n",
    "    for i in range (0, len(dataPoints)):\n",
    "        x = dataPoints[i,0]\n",
    "        y = dataPoints[i,1]\n",
    "        \n",
    "        gradientB += -(2/n) * (y - (m  * x) + b)\n",
    "        gradientM += -(2/n) * x * ((y - (m * x)) + b)\n",
    "    \n",
    "    adjustedB = b - (learning_rate * gradientB)\n",
    "    adjustedM = m - (learning_rate * gradientM)\n",
    "    return [adjustedB, adjustedM]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "Given dataset looking like following:\n",
    "\n",
    "    32.502345269453031,31.70700584656992\n",
    "    53.426804033275019,68.77759598163891\n",
    "\n",
    "In above dataset is in 'x,y' format. \n",
    "\n",
    "Dataset File: linear-regression-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all data points in (x,y) format\n",
    "dataPoints = genfromtxt(\"linear-regression-data.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-Parameters\n",
    "- Learning Rate is the stepping size that defines how fast of slow a model will converge.\n",
    "- Initializing initial value of slope and intercept.\n",
    "- The number of iteration we want our model to run to given best fitting values for slope and intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "iniB = 0\n",
    "iniM = 0\n",
    "noIteration = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL ERROR: 5565.107834\n",
      "INTERCEPT (b): 0.089899 and SLOPE (m):  1.481254\n",
      "FINAL ERROR: 112.645300\n"
     ]
    }
   ],
   "source": [
    "#Calculate initial error\n",
    "print(\"INITIAL ERROR: %f\" % error_function(iniB, iniM, dataPoints))\n",
    "\n",
    "#Iterate modal to train and adjust the value of slope (m) and intercept (b) over each iteration with gradient descent\n",
    "b = iniB\n",
    "m = iniM\n",
    "for i in range(noIteration):\n",
    "    b, m = gradient_descent(b, m, array(dataPoints), learning_rate)\n",
    "    \n",
    "    #Uncomment following line to see training progress\n",
    "    #print(\"SLOPE(m): %f | INTERCEPT(b): %f | ERROR: %f\" % (b, m, error_function(b, m, dataPoints)))\n",
    "\n",
    "#Training Result\n",
    "print(\"INTERCEPT (b): %f and SLOPE (m): % f\" % (b, m))\n",
    "print(\"FINAL ERROR: %f\" % error_function(b, m, dataPoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The initial error (5565.107834) was was reduced to (112.645300).\n",
    "\n",
    "Now, we have the value for slope(m) and intercept(b), i.e., we have the model (line) that defins this dataset such that if we are given an input feature (x) for this data set, we can predict the output (y).\n",
    "\n",
    "That's Linear Regression Model in Machine Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
