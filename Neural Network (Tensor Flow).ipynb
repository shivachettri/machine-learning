{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Flow\n",
    "TensorFlow is a Google's library that expresses everything into graph (expression/variables/operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PLACEHOLDER\n",
    "\n",
    "We will use data from MNIST example data from tensorflow.\n",
    "Input Dataset: 28px x 28px \n",
    "Output Label: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define operations in terms of expression and variable to be used in tenfow session below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FEED FORWARD\n",
    "\n",
    "We will be creat a neural network with 3 (input, hidden and output) layers. Therefore, we need to randomly initialize weights and biasis for 2 synapsis:\n",
    "- 1st synapsis connecting input layer and hidden layer.\n",
    "- 2nd synapsis connecting hidden layer and output layer.\n",
    "\n",
    "Then, we will calculate the weighted sum for 1st synapsis and pass it through 'relu' activation function and then we will calculate the weighted sum of 2nd synapsis and pass it through softmax function to get the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2h_weights = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name='InputToHiddenWeights')\n",
    "i2h_bias = tf.Variable(tf.random_normal([300]), name='InputToHiddenBias')\n",
    "\n",
    "h2o_weights = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='HiddenToOutputWeights')\n",
    "h2o_bias = tf.Variable(tf.random_normal([10]), name='HiddenToOutputBias')\n",
    "\n",
    "i2h_weighted_sum = tf.add(tf.matmul(x, i2h_weights), i2h_bias)\n",
    "i2h_activated_weighted_sum = tf.nn.relu(i2h_weighted_sum)\n",
    "\n",
    "predicted_y = tf.nn.softmax(tf.add(tf.matmul(i2h_activated_weighted_sum, h2o_weights), h2o_bias)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COST/LOSS FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_y = tf.clip_by_value(predicted_y, 1e-10, 0.9999999)\n",
    "cost_function = -tf.reduce_mean(tf.reduce_sum(y * tf.log(scaled_y) + (1 - y) * tf.log(1 - scaled_y), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(predicted_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to setup an object that will initialize all the afore-defined variables, operations and expression to be run inside tensorflow session below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TRAINING\n",
    "In tensorflow, all the operation happens inside tensorflow session. \n",
    "\n",
    "All the variables (i2h_weights, i2h_bias, h2o_weights and h2o_bias) and operation (gradient_descent, cost_function, accuracy, initializer, i2h_weighted_sum and i2h_activated_weighted_sum) defined above are run inside.\n",
    "\n",
    "NOTE: We will be using mini-batch gradient descent instead of batch descent for better performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "COST: 0.756044 \n",
      "COST: 0.264363 \n",
      "COST: 0.198879 \n",
      "COST: 0.166144 \n",
      "COST: 0.141082 \n",
      "COST: 0.123921 \n",
      "COST: 0.105480 \n",
      "COST: 0.093716 \n",
      "COST: 0.084787 \n",
      "COST: 0.074629 \n",
      "0.9733\n"
     ]
    }
   ],
   "source": [
    "#load the tensorflow's example dataset\n",
    "mnist_dataset = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "#tensor flow session begins\n",
    "with tf.Session() as SESSION:\n",
    "    #initializes all the global variables and operation defined previously\n",
    "    SESSION.run(initializer)\n",
    "    \n",
    "    #number of mini-batch (each batch of size: 100)\n",
    "    no_100_batch = int(len(mnist_dataset.train.labels) / 100)\n",
    "    \n",
    "    #loopr for 10 epoch\n",
    "    for iteration in range(10):\n",
    "        #loop through each mini-batch\n",
    "        for i in range(iteration):\n",
    "            #extract x_ and y_\n",
    "            x_, y_ = mnist_dataset.train.next_batch(batch_size=100)\n",
    "            #feed extract x_ and y_ into placeholder x and y\n",
    "            #and calculate the cost after each optimization with gradient descent\n",
    "            _, cost = SESSION.run([gradient_descent, cost_function], feed_dict={x: x_, y: y_})\n",
    "        \n",
    "        \n",
    "        print(\"COST: %f \" % (cost / no_100_batch))\n",
    "        \n",
    "    print(SESSION.run(accuracy, feed_dict={x: mnist_dataset.test.images, y: mnist_dataset.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how tensorflow rolls!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
